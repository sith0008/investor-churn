---
title: "Telco Churn Analysis"
output: pdf_document
---

# Import dependencies

```{r}
library(dplyr)
library(scales)
library(ggplot2)
library(corrplot)
library(gridExtra)
library(ggthemes)
library(caret)
library(MASS)
library(randomForest)
library(party)
library(MLmetrics)
library(rpart)
library(rpart.plot)
library(precrec)
```

# Read in data and preprocess

```{r}
telcoChurn <- read.csv(file = 'telco_churn_edited.csv')
telcoChurn = subset(telcoChurn, select=-c(customerID))
telcoChurn$Churn <- as.factor(telcoChurn$Churn)
telcoChurn <- na.omit(telcoChurn)

```

# Exploratory Data Analysis

## Tenure against Churn

```{r}
tenureCounts <- table(telcoChurn$tenure)
barplot(
  tenureCounts,
  main="Tenure Distribution",
  xlab="Tenure",
  col="skyblue",
  border=F
)
```

```{r}
# percentage exited for each tenure
tenure <- telcoChurn %>% count(Churn, tenure)
tenure$Churn <- as.factor(tenure$Churn)
tenure$Churn <- ifelse(tenure$Churn == "No", "No Churn", "Churned")

ggplot(tenure, aes(fill=Churn, y=n, x=tenure))+
  geom_bar(position="fill", stat="identity")+
  scale_fill_manual(values = c("skyblue", scales::alpha("red", .5)))

```

```{r}
# boxplot for tenure against churn
boxplot(
  tenure ~ Churn, 
  data=telcoChurn, 
  main="Tenure against Churn",
  xlab="Churned",
  ylab="Tenure"
  )
```

## Contract Type against Churn

```{r}
ContractType <- table(telcoChurn$Contract)
barplot(
  ContractType,
  main="Contract Type Distribution",
  xlab="Contract",
  col="skyblue",
  border=F
)
```

```{r}
# percentage exited for each contract type
Contract <- telcoChurn %>% count(Churn, Contract)
Contract$Churn <- as.factor(Contract$Churn)
Contract$Churn <- ifelse(Contract$Churn == "No", "No Churn", "Churned")

ggplot(Contract, aes(fill=Churn, y=n, x=Contract))+
  geom_bar(position="fill", stat="identity")+
  scale_fill_manual(values = c("skyblue", scales::alpha("red", .5)))
```

## Monthly Charges against Churn

```{r}
MonthlyChargesCounts <- table(telcoChurn$MonthlyChargesIntervals)
barplot(
  MonthlyChargesCounts,
  main="Monthly Charges Distribution",
  xlab="Monthly Charges",
  col="skyblue",
  border=F
)
```

```{r}
# percentage exited for Montly Charge Range
MonthlyChargesIntervals <- telcoChurn %>% count(Churn, MonthlyChargesIntervals)
MonthlyChargesIntervals$Churn <- as.factor(MonthlyChargesIntervals$Churn)
MonthlyChargesIntervals$Churn <- ifelse(MonthlyChargesIntervals$Churn == "No", "No Churn", "Churned")


ggplot(MonthlyChargesIntervals, aes(fill=Churn, y=n, x=MonthlyChargesIntervals))+
  geom_bar(position="fill", stat="identity")+
  scale_fill_manual(values = c("skyblue", scales::alpha("red", .5)))
```

```{r}
# boxplot for monthly charges against churn
boxplot(
  MonthlyCharges ~ Churn, 
  data=telcoChurn, 
  main="Monthly Charges against Churned",
  xlab="Churned",
  ylab="Monthly Charges"
  )
```

## Total Charges against Churn

```{r}
TotalChargesCounts <- table(telcoChurn$TotalChargesIntervals)
barplot(
  TotalChargesCounts,
  main="Total Charges Distribution",
  xlab="Total Charges",
  col="skyblue",
  border=F
)
```

```{r}
# percentage exited for Montly Charge Range
TotalChargesIntervals <- telcoChurn %>% count(Churn, TotalChargesIntervals)
TotalChargesIntervals$Churn <- as.factor(TotalChargesIntervals$Churn)
TotalChargesIntervals$Churn <- ifelse(TotalChargesIntervals$Churn == "No", "No Churn", "Churned")


ggplot(TotalChargesIntervals, aes(fill=Churn, y=n, x=TotalChargesIntervals))+
  geom_bar(position="fill", stat="identity")+
  scale_fill_manual(values = c("skyblue", scales::alpha("red", .5)))
```

## Senior Citizen against Churn

```{r}
SeniorCitizenscount <- table(telcoChurn$SeniorCitizen)
barplot(
  SeniorCitizenscount,
  main="Senior Citizen Ratio",
  xlab="Senior Citizen",
  col="skyblue",
  border=F
)
```

```{r}
# percentage exited for Montly Charge Range
SeniorCitizen <- telcoChurn %>% count(Churn, SeniorCitizen)
SeniorCitizen$Churn <- as.factor(SeniorCitizen$Churn)
SeniorCitizen$Churn <- ifelse(SeniorCitizen$Churn == "No", "No Churn", "Churned")


ggplot(SeniorCitizen, aes(fill=Churn, y=n, x=SeniorCitizen))+
  geom_bar(position="fill", stat="identity")+
  scale_fill_manual(values = c("skyblue", scales::alpha("red", .5)))
```

# Training

## Train-test-split

```{r}
# train test split
idx = createDataPartition(telcoChurn$Churn, p=0.7, list=FALSE)
set.seed(42)
train = telcoChurn[idx,]
test = telcoChurn[-idx,]

train$Churn = ifelse(train$Churn == "Yes",1,0)
test$Churn = ifelse(test$Churn == "Yes",1,0)
train$Churn = as.factor(train$Churn)
test$Churn = as.factor(test$Churn)

dim(train); dim(test)
```

## Logistic Regression

```{r}
# logistic regression to predict churn
logreg = glm(Churn ~ .,
             family=binomial(link="logit"),
             data=train)

print(summary(logreg))

```

### Feature importance using deviance

```{r}
# feature importance: the steeper the drop in deviance the more important the feature
anova(logreg, test="Chisq")
```

### Accuracy

```{r}
# evaluating logistic regression model against test data
logreg.scores <- predict(logreg,newdata=test,type='response')
logreg.pred <- ifelse(logreg.scores > 0.5,1,0)
misclassError <- mean(logreg.pred != test$Churn)
print(paste('Logistic Regression Accuracy',1-misclassError))
```

### Confusion Matrix

```{r}
print("Confusion Matrix for Logistic Regression"); table(Predicted = logreg.pred, Actual = test$Churn)
```

### ROC, PRC curves

```{r}
precrec.logreg <- evalmod(scores = logreg.scores, labels = test$Churn)
print(precrec.logreg)
```

```{r}
autoplot(precrec.logreg)
```

## Decision Tree

```{r}
cart <- rpart(Churn ~ ., 
              data=train, 
              method = 'class', 
              control=rpart.control(minsplit = 2, cp = 0)
              )
plotcp(cart)
```

### Optimisation of CP value

```{r}
CVerror.cap <- cart$cptable[which.min(cart$cptable[,"xerror"]), "xerror"] + cart$cptable[which.min(cart$cptable[,"xerror"]), "xstd"]

# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree cart1.
i <- 1; j<- 4
while (cart$cptable[i,j] > CVerror.cap) {
  i <- i + 1
}

# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(cart$cptable[i,1] * cart$cptable[i-1,1]), 1)
```

```{r}
cart.opt <- prune(cart, cp = cp.opt)
cart.opt$variable.importance

```

### Accuracy

```{r}
tree.pred <- predict(cart.opt, test, type="class")
tree.pred.scores <- predict(cart.opt, test, type="prob")
table.pred <- table(Predicted = tree.pred, Actual = test$Churn)
print(paste('Decision Tree Accuracy',sum(diag(table.pred))/sum(table.pred)))
```

### Confusion Matrix

```{r}
# Note: accuracy is 80% because of unbalanced dataset; most data points have Churn = 0. From the confusion matrix, we can see that there are a lot of false negatives
print("Confusion Matrix for Decision Tree"); table(Predicted = tree.pred, Actual = test$Churn)
```

### ROC, PRC curves

```{r}
precrec.tree <- evalmod(scores = tree.pred.scores[, 2], labels = test$Churn)
print(precrec.tree)
```

```{r}
autoplot(precrec.tree)
```

## Random Forest 

```{r}
rfmodel <- randomForest(Churn ~ ., data = train, proximity = TRUE)
```

```{r}
rfmodel
```

### OOB error plot for initial model

```{r}
# model based on err.rate matrix: [OOB, No, Yes]
oob.error.data <- data.frame(Trees=rep(1:nrow(rfmodel$err.rate), times=3), Type=rep(c("OOB", "0", "1"), each=nrow(rfmodel$err.rate)), Error=c(rfmodel$err.rate[,"OOB"], rfmodel$err.rate[,"0"], rfmodel$err.rate[,"1"]))

ggplot(data=oob.error.data, aes(x=Trees, y=Error))+geom_line(aes(color=Type))

```

### Accuracy for initial model

```{r}
rfmodel.pred <- predict(rfmodel, test, type="response")
rfmodel.pred.scores <- predict(rfmodel, test, type="prob")
table.rfmodel.pred <- table(Predicted = rfmodel.pred, Actual = test$Churn)
print(paste('Random Forest Accuracy',sum(diag(table.rfmodel.pred))/sum(table.rfmodel.pred)))
```

### Confusion Matrix for initial model

```{r}
print("Confusion Matrix for Random Forest"); table(Predicted = rfmodel.pred, Actual = test$Churn)
```

### ROC, PRC curves for initial model

```{r}
precrec.rf <- evalmod(scores = rfmodel.pred.scores[, 2], labels = test$Churn)
print(precrec.rf)
```

### Optimisation of no. of variable in each internal node

```{r}
# optimize no. of variables at each internal node in tree
oob.values <- vector(length=10)
for(i in 1:10){
  temp.model <- randomForest(Churn ~ ., data = telcoChurn, mtry=i, ntree=1000)
  
  #store OOB error rate for each random forest that uses diff value of i
  oob.values[i] <- temp.model$err.rate[nrow(temp.model$err.rate),1]
}

oob.values
```

```{r}
# no. of variables = 2 gives lowest oob err.rate

rfmodeloptim <- randomForest(Churn ~ ., data = train, mtry=2, proximity = TRUE, type='classification')
```

```{r}
rfmodeloptim
# error rate reduced
```

### Accuracy for optimised model

```{r}
rfmodeloptim.pred <- predict(rfmodeloptim, test, type="response")
rfmodeloptim.pred.scores <- predict(rfmodeloptim, test, type="prob")
table.rfmodeloptim.pred <- table(Predicted = rfmodeloptim.pred, Actual = test$Churn)
print(paste('Random Forest Accuracy',sum(diag(table.rfmodeloptim.pred))/sum(table.rfmodeloptim.pred)))
```

### Confusion Matrix for optimised model

```{r}
print("Confusion Matrix for Random Forest"); table(Predicted = rfmodeloptim.pred, Actual = test$Churn)
```

### ROC, PRC curves for optimised model

```{r}
precrec.rf <- evalmod(scores = rfmodeloptim.pred.scores[, 2], labels = test$Churn)
print(precrec.rf)
```

```{r}
autoplot(precrec.rf)
```

### OOB error plot for optimised model

```{r}
oob.error.data <- data.frame(Trees=rep(1:nrow(rfmodel$err.rate), times=3), Type=rep(c("OOB", "0", "1"), each=nrow(rfmodeloptim$err.rate)), Error=c(rfmodeloptim$err.rate[,"OOB"], rfmodeloptim$err.rate[,"0"], rfmodeloptim$err.rate[,"1"]))

ggplot(data=oob.error.data, aes(x=Trees, y=Error))+geom_line(aes(color=Type))
```

# Feature Importances across models

```{r}
imp.logreg <- varImp(logreg, scale = FALSE)
imp.logreg
```

```{r}
imp.tree <- varImp(cart.opt, scale = FALSE)
imp.tree
```

```{r}
imp.rfmodeloptim <- varImp(rfmodeloptim, scale = FALSE)
imp.rfmodeloptim
```


