---
title: "Bank Churn"
output:
  pdf_document: default
  html_notebook: default
---

```{r}
library(dplyr)
library(scales)
library(ggplot2)
library(corrplot)
library(gridExtra)
library(ggthemes)
library(caret)
library(MASS)
library(randomForest)
library(party)
library(MLmetrics)
library(rpart)
library(rpart.plot)
library(precrec)

```


```{r}
bankChurn <- read.csv(file = 'Churn_Modelling.csv')
bankChurn = subset(bankChurn, select=-c(Surname, CustomerId, RowNumber, CreditScore, Geography, Gender, HasCrCard))
names(bankChurn)[names(bankChurn) == "Exited"] <- "Churn"
bankChurn$Churn = ifelse(bankChurn$Churn == 1, "Yes", "No")
bankChurn$Churn = as.factor(bankChurn$Churn)
dim(bankChurn)
```

```{r}
tenureCounts <- table(bankChurn$Tenure)
barplot(
  tenureCounts, 
  main="Tenure Distribution",
  xlab="Tenure",
  col="skyblue",
  border=F
  )
```

```{r}
# percentage Churn for each tenure
tenure <- bankChurn %>% count(Churn, Tenure)

ggplot(tenure, aes(fill=Churn, y=n, x=Tenure))+ 
  ggtitle("Churn percentage against tenure") +
  geom_bar(position="fill", stat="identity") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values=c("skyblue", scales::alpha("red", .5)))

```
```{r}
# boxplot for tenure against Churn
boxplot(
  Tenure ~ Churn, 
  data=bankChurn, 
  main="Tenure against Churn",
  xlab="Churn",
  ylab="Tenure"
  )
```

```{r}
# age distribution
hist(
  main="Age distribution", 
  xlab="Age", 
  bankChurn$Age, 
  xlim=c(10,80), 
  col='skyblue',
  border=F)
```

```{r}
# boxplot for age against Churn
boxplot(
  Age ~ Churn, 
  data=bankChurn, 
  main="Age against Churn",
  xlab="Churn",
  ylab="Age"
  )
```

```{r}
# balance distribution
hist(
  main="Balance distribution", 
  xlab="Balance", 
  bankChurn$Balance, 
  xlim=c(0,200000), 
  col='skyblue',
  border=F)
```

```{r}
# boxplot for balance against Churn
boxplot(
  Balance ~ Churn, 
  data=bankChurn, 
  main="Balance against Churn",
  xlab="Churn",
  ylab="Balance"
  )
```

```{r}
# salary distribution
hist(
  main="Salary distribution", 
  xlab="Salary", 
  bankChurn$EstimatedSalary, 
  xlim=c(0,200000), 
  col='skyblue',
  border=F)
```

```{r}
# boxplot for salary against Churn
boxplot(
  EstimatedSalary ~ Churn, 
  data=bankChurn, 
  main="Salary against Churn",
  xlab="Churn",
  ylab="Salary"
  )
```

```{r}
# percentage Churn against is_active
isActive <- bankChurn %>% count(Churn, IsActiveMember)
ggplot(isActive, aes(fill=Churn, y=n, x=IsActiveMember))+ 
  ggtitle("Churn percentage against activity")+
  geom_bar(position="fill", stat="identity")+
  theme(plot.title = element_text(hjust = 0.5))+
  scale_fill_manual(values=c("skyblue", scales::alpha("red", .5)))
```



```{r}
numProdCounts <- table(bankChurn$NumOfProducts)
barplot(
  numProdCounts, 
  main="Number of products Distribution",
  xlab="Number of products",
  col="skyblue",
  border=F
  )
```

```{r}
# percentage Churn against number of products
numProducts <- bankChurn %>% count(Churn, NumOfProducts)

ggplot(numProducts, aes(fill=Churn, y=n, x=NumOfProducts))+ 
  ggtitle("Churn percentage against number of products")+
  geom_bar(position="fill", stat="identity")+
  theme(plot.title = element_text(hjust = 0.5))+
  scale_fill_manual(values=c("skyblue", scales::alpha("red", .5)))
```
```{r}
# train test split
idx = createDataPartition(bankChurn$Churn, p=0.7, list=FALSE)
set.seed(42)
train = bankChurn[idx,]
test = bankChurn[-idx,]
train$Churn = ifelse(train$Churn == "Yes",1,0)
test$Churn = ifelse(test$Churn == "Yes",1,0)
train$Churn = as.factor(train$Churn)
test$Churn = as.factor(test$Churn)


dim(train); dim(test)
```

```{r}
# logistic regression to predict churn
logreg = glm(Churn ~ ., 
             family=binomial(link="logit"),
             data=train
             )
print(summary(logreg))
```
```{r}
# feature importance: the steeper the drop in deviance the more important the feature
anova(logreg, test="Chisq")
```
```{r}
# evaluating logistic regression model against test data
logreg.pred <- predict(logreg,newdata=test,type='response')
logreg.pred <- ifelse(logreg.pred > 0.5,1,0)
misclassError <- mean(logreg.pred != test$Churn)
print(paste('Logistic Regression Accuracy',1-misclassError))
```
```{r}
print("Confusion Matrix for Logistic Regression")
table(Predicted = logreg.pred, Actual = test$Churn)
```
```{r}
precrec.logreg <- evalmod(scores = logreg.pred, labels = test$Churn)
print(precrec.logreg)
```
```{r}
autoplot(precrec.logreg)
```

```{r}
cart <- rpart(Churn ~ ., 
              data=train, 
              method = 'class', 
              control=rpart.control(minsplit = 2, cp = 0)
              )
plotcp(cart)
```
```{r}
CVerror.cap <- cart$cptable[which.min(cart$cptable[,"xerror"]), "xerror"] + cart$cptable[which.min(cart$cptable[,"xerror"]), "xstd"]

# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree cart1.
i <- 1; j<- 4
while (cart$cptable[i,j] > CVerror.cap) {
  i <- i + 1
}

# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(cart$cptable[i,1] * cart$cptable[i-1,1]), 1)
```

```{r}
cart.opt <- prune(cart, cp = cp.opt)
cart.opt$variable.importance

```


```{r}
tree.pred <- predict(cart.opt, test, type="class")
table.pred <- table(Predicted = tree.pred, Actual = test$Churn)
print(paste('Decision Tree Accuracy',sum(diag(table.pred))/sum(table.pred)))
```
```{r}
# Note: accuracy is 80% because of unbalanced dataset; most data points have Churn = 0. From the confusion matrix, we can see that there are a lot of false negatives
print("Confusion Matrix for Decision Tree"); table(Predicted = tree.pred, Actual = test$Churn)
```

```{r}
precrec.tree <- evalmod(scores = c(tree.pred), labels = test$Churn)
print(precrec.tree)
```
```{r}
autoplot(precrec.tree)
```
```{r}
rf <- randomForest(Churn ~ ., data = train, proximity = TRUE, type='classification')

```

```{r}
rf
```
```{r}
# model based on err.rate matrix: [OOB, No, Yes]
oob.error.data <- data.frame(Trees=rep(1:nrow(rf$err.rate), times=3), Type=rep(c("OOB", "0", "1"), each=nrow(rf$err.rate)), Error=c(rf$err.rate[,"OOB"], rf$err.rate[,"0"], rf$err.rate[,"1"]))

ggplot(data=oob.error.data, aes(x=Trees, y=Error))+geom_line(aes(color=Type))
```

```{r}
# optimize no. of variables at each internal node in tree
oob.values <- vector(length=10)
for(i in 1:10){
  temp.model <- randomForest(Churn ~ ., data = train, mtry=i, ntree=1000)
  
  #store OOB error rate for each random forest that uses diff value of i
  oob.values[i] <- temp.model$err.rate[nrow(temp.model$err.rate),1]
}

oob.values
```

```{r}
# no. of variables = 2 gives lowest oob err.rate

rfmodeloptim <- randomForest(Churn ~ ., data = train, mtry=2, proximity = TRUE, type='classification')
```
```{r}
rfmodeloptim
# error rate reduced
```

```{r}
rf.pred <- predict(rfmodeloptim, test)
table.rf.pred <- table(Predicted = rf.pred, Actual = test$Churn)
print(paste('Random Forest Accuracy',sum(diag(table.rf.pred))/sum(table.rf.pred)))
```
```{r}
print("Confusion Matrix for Random Forest"); table(Predicted = rf.pred, Actual = test$Churn)
```

```{r}
precrec.rf <- evalmod(scores = c(rf.pred), labels = test$Churn)
print(precrec.rf)
```

```{r}
autoplot(precrec.rf)
```


```{r}
imp.logreg <- varImp(logreg, scale = FALSE)
imp.logreg
```

```{r}
imp.tree <- varImp(cart.opt, scale = FALSE)
imp.tree
```

```{r}
imp.rf <- varImp(rfmodeloptim, scale = FALSE)
imp.rf
```

